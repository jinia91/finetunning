from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from sklearn.metrics import accuracy_score


# 1. ë°ì´í„°ì…‹ ë¡œë“œ
dataset = load_dataset("imdb", split="train[:100]").train_test_split(test_size=0.2)
sample = dataset["train"][7]
# {'text': "Luise Rainer received an Oscar for her performance in The Good Earth. Unfortunately, her role required no. She did not say much and looked pale throughout the film. Luise's character was a slave then given away to marriage to Paul Muni's character (he did a fantastic job for his performance). Set in ancient Asia, both actors were not Asian, but were very convincing in their roles. I hope that Paul Muni received an Oscar for his performance, because that is what Luise must have gotten her Oscar for. She must have been a breakthrough actress, one of the first to method act. This seems like something that Hollywood does often. Al Pacino has played an Italian and Cuban. I felt Luise's performance to be lackluster throughout, and when she died, she did not change in expression from any previous scenes. She stayed the same throughout the film; she only changed her expression or emotion maybe twice. If her brilliant acting was so subtle, I suppose I did not see it.", 'label': 0}
print(sample)
print(f"ğŸ“ ë¦¬ë·° ë‚´ìš©:\n{sample['text']}\n")
print(f"ğŸ·ï¸ ë¼ë²¨ (0=ë¶€ì •, 1=ê¸ì •): {sample['label']}")


# âœ… 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
print("Loading tokenizer and model...")
tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased")

def tokenize(batch):
    return tokenizer(batch["text"], truncation=True, padding=True)

dataset = dataset.map(tokenize, batched=True)


# âœ… 4. í›ˆë ¨ ì„¤ì • ë° Trainer êµ¬ì„±
# ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜
print("Setting up training arguments and trainer...")
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=-1)
    acc = accuracy_score(labels, predictions)
    return {"accuracy": acc}

args = TrainingArguments(
    output_dir="test",  # ëª¨ë¸ì´ ì €ì¥ë  ê²½ë¡œ
    per_device_train_batch_size=8,  # í•™ìŠµ ì‹œ ë°°ì¹˜ í¬ê¸° (GPU í•˜ë‚˜ë‹¹ 8ê°œì”© ì²˜ë¦¬)
    num_train_epochs=15,  # ì—í­ ìˆ˜ (ë°ì´í„°ì…‹ì„ 15ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµ)
     report_to = "none",  # wandb, tensorboard ë“± ì™¸ë¶€ ë¡œê¹… íˆ´ ë¹„í™œì„±í™”
        logging_steps=1                   # ğŸ’¡ ëª‡ stepë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥í• ì§€
)
trainer = Trainer(
    model=model,  # ì‚¬ìš©í•  ëª¨ë¸
    args=args,  # í›ˆë ¨ ì„¤ì •ê°’
    train_dataset=dataset["train"],  # í•™ìŠµìš© ë°ì´í„°ì…‹
    eval_dataset=dataset["test"],  # ê²€ì¦ìš© ë°ì´í„°ì…‹
    compute_metrics=compute_metrics  # ğŸ’¡ ì •í™•ë„ ê³„ì‚° í•¨ìˆ˜ ì¶”ê°€
)

# âœ… 5. íŒŒì¸íŠœë‹ ì‹¤í–‰ (ëª¨ë¸ í•™ìŠµ)
print("Starting training...")
trainer.train()

# âœ… 6. ëª¨ë¸ í‰ê°€
print("Evaluating model...")
results = trainer.evaluate()
print(results)

# âœ… 7. ëª¨ë¸ ì €ì¥
print("Saving model...")
model.save_pretrained("fine_tuned_model")
