import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling
from peft import get_peft_model, LoraConfig, TaskType  # LoRA ê¸°ë°˜ íŒŒì¸íŠœë‹ ì§€ì›
import numpy as np

model_id = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.pad_token = tokenizer.eos_token
model = AutoModelForCausalLM.from_pretrained(model_id)


# ì…ë ¥ ë¬¸ì¥ì„ ìˆ«ìë¡œ ë°”ê¿”ì£¼ëŠ” tokenizerë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
# return_tensors="pt"ëŠ” PyTorch í…ì„œ í˜•íƒœë¡œ ë°˜í™˜í•˜ê² ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

lora_config = LoraConfig(
    r=16,  # ë­í¬: LoRA ë‚´ë¶€ ì°¨ì›. ì‘ì„ìˆ˜ë¡ ê°€ë³ê³  ë¹ ë¦„
    lora_alpha=8,  # í•™ìŠµ ì•ˆì •ì„±ì„ ìœ„í•œ ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜
    target_modules=["c_attn","c_proj","q_attn"],  # GPT-2ì—ì„œ LoRAë¥¼ ì ìš©í•  ë ˆì´ì–´. 'c_attn'ì€ Attention ëª¨ë“ˆì˜ í•µì‹¬ ë¶€ë¶„ì…ë‹ˆë‹¤.
    # ğŸ” GPT-2ì˜ ë ˆì´ì–´ë“¤
    # - "c_attn": ì¿¼ë¦¬, í‚¤, ë°¸ë¥˜ë¥¼ ìƒì„±í•˜ëŠ” í•µì‹¬ ì–´í…ì…˜ ì…ë ¥ ë ˆì´ì–´
    # - "c_proj": ì–´í…ì…˜ ì¶œë ¥ ë²¡í„°ë¥¼ ë³€í™˜í•˜ëŠ” íˆ¬ì˜ ë ˆì´ì–´
    # - "q_attn": self-attention ì¤‘ ì¿¼ë¦¬ ì—°ì‚°ì— ê´€ì—¬í•˜ëŠ” ë¶€ë¶„ (GPT-J ë“± ì¼ë¶€ êµ¬ì¡°ì— ì¡´ì¬)
    # - "mlp.c_fc": í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬ì˜ ì²« ë²ˆì§¸ ì„ í˜• ê³„ì¸µ (MLPì˜ ì…ë ¥ ë¶€ë¶„)
    # - "mlp.c_proj": MLPì˜ ì¶œë ¥ ë¶€ë¶„
    lora_dropout=0.05,  # ë“œë¡­ì•„ì›ƒ ì ìš© (ê³¼ì í•© ë°©ì§€)
    bias="none",  # bias íŒŒë¼ë¯¸í„°ëŠ” í•™ìŠµí•˜ì§€ ì•ŠìŒ
    task_type=TaskType.CAUSAL_LM,  # ì‘ì—… ìœ í˜•: ì–¸ì–´ ìƒì„± (Causal Language Modeling)
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

data = {
    "text": [
        "### ì§ˆë¬¸: ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?\n### ë‹µë³€: ìˆœë‘¥ì´",
        "### ì§ˆë¬¸: ë°”ë‹¤ëŠ” ì™œ íŒŒë€ê°€ìš”?\n### ë‹µë³€: í–‡ë¹›ì˜ ì‚°ë€",
    ]
}
dataset = Dataset.from_dict(data)

def tokenize_function(example):
    return tokenizer(example["text"], padding="max_length", truncation=True, max_length=128)
tokenized_dataset = dataset.map(tokenize_function)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
training_args = TrainingArguments(
    output_dir="../results",  # ê²°ê³¼ ë””ë ‰í† ë¦¬
    per_device_train_batch_size=10,  # GPUë‚˜ CPU 1ê°œë‹¹ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
    num_train_epochs=50,  # ë°ì´í„° ì „ì²´ë¥¼ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí• ì§€ (ì—¬ê¸°ì„  10ë²ˆ)
    logging_steps=1,  # 1ìŠ¤í…ë§ˆë‹¤ ë¡œê·¸ ì¶œë ¥ (í•™ìŠµ ì§„í–‰ ìƒí™© í™•ì¸)
    save_strategy="no",  # ì¤‘ê°„ ì €ì¥ ìƒëµ (ë°ëª¨ ëª©ì ì´ë¯€ë¡œ)
    fp16=False,  # GPU ì—†ì´ CPUë¡œ í•™ìŠµ ì‹œ False ì„¤ì •,
         report_to = "none",  # wandb, tensorboard ë“± ì™¸ë¶€ ë¡œê¹… íˆ´ ë¹„í™œì„±í™”
)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    # padding ë¶€ë¶„ì€ ë¬´ì‹œ
    mask = labels != -100
    correct = (predictions == labels) & mask
    accuracy = correct.sum() / mask.sum()

    return {"accuracy": accuracy}
trainer = Trainer(
    model=model,  # í•™ìŠµí•  ëª¨ë¸
    args=training_args,  # í•™ìŠµ ì„¤ì •
    train_dataset=tokenized_dataset,  # í•™ìŠµ ë°ì´í„°
    tokenizer=tokenizer,  # í…ìŠ¤íŠ¸ ë””ì½”ë”©ìš©
    data_collator=data_collator,  # ë°°ì¹˜ êµ¬ì„± ë„ìš°ë¯¸
    compute_metrics=compute_metrics  # <-- ì—¬ê¸° ì¶”ê°€

)

# í•™ìŠµ ì‹œì‘!
trainer.train()


# "ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?"ì´ë¼ëŠ” ì§ˆë¬¸ì— ë‹µì„ í•˜ë„ë¡ êµ¬ì„±í•©ë‹ˆë‹¤.
input_text = "### ì§ˆë¬¸:ìš°ë¦¬ì§‘ ê°•ì•„ì§€ ì´ë¦„ì€?\n### ë‹µë³€:"

# ì…ë ¥ ë¬¸ì¥ì„ ìˆ«ìë¡œ ë°”ê¿”ì£¼ëŠ” tokenizerë¥¼ ì‚¬ìš©í•´ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
# return_tensors="pt"ëŠ” PyTorch í…ì„œ í˜•íƒœë¡œ ë°˜í™˜í•˜ê² ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.
inputs = tokenizer(input_text, return_tensors="pt")

# ëª¨ë¸ì´ ì˜¬ë¼ê°€ ìˆëŠ” ë””ë°”ì´ìŠ¤(GPU ë˜ëŠ” CPU)ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
device = model.device

# ì…ë ¥ ë°ì´í„°ë„ ëª¨ë¸ì´ ìˆëŠ” ë””ë°”ì´ìŠ¤ë¡œ ì˜®ê²¨ì¤ë‹ˆë‹¤.
# ê·¸ë˜ì•¼ ëª¨ë¸ê³¼ ë°ì´í„°ê°€ ê°™ì€ ì¥ì¹˜ì— ìˆì–´ ì—°ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
inputs = {k: v.to(device) for k, v in inputs.items()}

# ëª¨ë¸ì—ê²Œ ë‹µë³€ì„ ìƒì„±í•˜ë„ë¡ ì§€ì‹œí•©ë‹ˆë‹¤.
# max_new_tokens=50ì€ ìµœëŒ€ 50ê°œì˜ ìƒˆë¡œìš´ ë‹¨ì–´(í† í°)ë¥¼ ìƒì„±í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
outputs = model.generate(**inputs, max_new_tokens=50)

# ìƒì„±ëœ ë‹µë³€ì„ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ ë°”ê¿”ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.
# skip_special_tokens=TrueëŠ” ì‹œì‘/ì¢…ë£Œ ê°™ì€ íŠ¹ìˆ˜ ê¸°í˜¸ëŠ” ì¶œë ¥í•˜ì§€ ì•Šê² ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
print(tokenizer.decode(outputs[0], skip_special_tokens=True))



